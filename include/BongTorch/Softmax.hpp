#pragma once

#include "Core.hpp" 
#include <memory>
// nb::max, nb::exp, nb::sum í•¨ìˆ˜ê°€ ì •ì˜ëœ NumBong.hpp íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.
// #include "NumBong.hpp" 

namespace bs { // ğŸ’¡ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ bs ì¶”ê°€

class Softmax : public Function { // bs::Functionì—ì„œ Functionìœ¼ë¡œ ìˆ˜ì •
private:
    int axis_;
public:
    // Softmax Functionì˜ ìƒì„±ì: Softmaxë¥¼ ê³„ì‚°í•  ì¶•(axis)ì„ ë°›ìŠµë‹ˆë‹¤.
    explicit Softmax(int axis = -1) : axis_(axis) {}

    // Function::forward ì˜¤ë²„ë¼ì´ë”© (TensorData íƒ€ì…ìœ¼ë¡œ í†µì¼)
    std::vector<TensorData> forward(const std::vector<TensorData>& xs) override {
        const TensorData& x = xs[0]; // ì…ë ¥ í…ì„œ (TensorDataë¡œ í†µì¼)

        // 1. ì•ˆì •í™” ë‹¨ê³„ (Stability): xì—ì„œ ìµœëŒ€ê°’ì„ ë¹¼ì¤ë‹ˆë‹¤.
        // nb::max(tensor, axis, keep_dims=true)ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.
        TensorData x_max = nb::max(x, axis_, true); 
        TensorData x_shifted = x - x_max; // ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ í†µí•´ xì˜ ê° ìš”ì†Œì—ì„œ ìµœëŒ€ê°’(x_max)ì„ ëºŒ

        // 2. ë¶„ì ê³„ì‚°: exp(x_shifted)
        TensorData numerator = nb::exp(x_shifted); 
        
        // 3. ë¶„ëª¨ ê³„ì‚°: sum(numerator)
        // nb::sum(tensor, axis, keep_dims=true)ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.
        TensorData denominator = nb::sum(numerator, axis_, true); 
        
        // 4. ìµœì¢… ê³„ì‚°: numerator / denominator
        TensorData y = numerator / denominator;

        return { y };
    }
    
    // NOTE: backwardëŠ” í•™ìŠµ ì‹œ í•„ìˆ˜ì ì´ì§€ë§Œ, í˜„ì¬ ì¶”ë¡  ì „ìš©ì„ ê°€ì •í•˜ê³  ìƒëµí•©ë‹ˆë‹¤.
};

// Softmax Functionì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í—¬í¼ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
inline std::shared_ptr<Variable> softmax(const std::shared_ptr<Variable>& x, int axis = -1) {
    auto f = std::make_shared<Softmax>(axis);
    // (*f) ì˜¤ë²„ë¡œë”©ì„ í†µí•´ Function í˜¸ì¶œ
    auto outs = (*f)(std::vector<std::shared_ptr<Variable>>{x});
    return outs[0]; 
}

} // namespace bs